---
layout: post
title: "dotScale 2015"
tags: dotscale
---

Pour la deuxième année consécutive, je suis allé faire un tour à [dotScale][1].
Toujours au même endroit, au [Théâtre de Paris][2], dans une belle salle de
théâtre (même si il y faisait un peu chaud dans l'aprem). Ça fait plaisir de
voir autant de gens réunis pour parler scalabilité (si seulement autant de
personnes venaient parler [CSS][3]...).

Il y avait en tout environ 750 personnes, et comme à son habitude l'organisation
était au top, les buffets bien remplis et très bons. Comme à son habitude,
dotScale n'émets pas de wifi dans la salle de conférences pour que les gens se
concentrent sur les talks (on nous incite même à ne pas utiliser nos laptops,
mais j'ai bravé l'interdit pour prendre les notes qui m'ont permis d'écrire cet
article).

Comme d'habitude aussi, j'y [croise][4] [du][5] [beau][6] [monde][7] [que][8]
[je][9] [connais][10] [déjà][11], mais c'est toujours agréable de discuter.

Je suis resté un peu sur ma faim en milieu de journée quand la conférence s'est
mise à devenir un `dotNoSQL` plutôt qu'un `dotScale`, mais les derniers talks
sont revenus sur la bonne piste donc j'en sors finalement content.

# Automatic Unattended Reboots

Le premier talk de la journée, de [Matt Bostock][12] de [gov.uk][13] commence
directement à nous parler de reboots automatiques. Matt travaille donc pour le
gouvernement du Royaume Uni et l'availability des différents sites de ministères
et ambassades.  Ils ont environ 12 millions de visiteurs uniques par semaine,
à toute heure du jour et la nuit (voyageurs, expatriés à l'autre bout du globe
qui utilisent les sites des ambassades).

Bien souvent quand on fait une mise à jour, il est nécessaire de faire un
reboot. Parfois, un simple restart des services qui vont bien suffit, mais c'est
pas toujours évident à faire. D'une part c'est un peu plus manuel qu'un bon
vieux reboot, et parfois ça ne suffit même pas (comme quand on mets à jour des
lib ssl -[heartbleed][14], anyone ?-).

Du coup, rebooter manuellement tout ses serveurs de sa batterie, ça va bien deux
minutes, mais il y a de bons moyens de rendre ça moins pénible. [Ubuntu][15],
qu'ils utilisent, est configuré pour mettre automatiquement à jour les paquets
qui contiennent des updates de sécurité (`cron` qui check toutes les
demi-heures de minuit à 9h du matin).

Comme je le disais plus haut, c'est déjà pas mal, mais parfois il faut faire un
reboot après ça. Dans ce cas, il est possible de spécifier un
`Unattended-Upgrade::Automatice-reboot "true"` pour que le serveur reboot dans
la foulée. Il faut par contre faire attention à ce que tous les serveurs ne
rebootent pas en même temps, en décalant légèrement le moment où les `cron`
tournent entre plusieurs machines.

Ils ont donc mis au point un système automatique pour s'assurer que tous les
reboots se fassent correctement.

Leur script [bash][16] tourne toutes les minutes, de minuit à 9h. Il check si
`/var/run/boot-required` existe (ce fichier est ajouté par Ubuntu lorsqu'une
update nécessite un reboot, c'est ce fichier qui est la cause du message qu'on
voit parfois quand on se loggue sur un serveur qui a besoin d'être rebooté).

Si c'est le cas, il check un flag dans [Puppet][17] qui indique si cette machine
a le droit de se rebooter seule. Certaines machines nécessitent un reboot
manuel, et dans ce cas la config Puppet s'assure du feature flipping. Ils sont
en train de modifier tout leurs serveurs pour qu'ils soient tous capables de se
rebooter, mais comme ils n'en sont pas encore là, la feature peut être
désactivée au cas par cas.

Ils requêtent ensuite [Icinga][18] (un fork de [Nagios][19] qui expose un
endpoint en json avec l'état actuel de santé des machines) pour voir si le
cluster sur lequel tourne le serveur est en bonne santé. S'il y a un trop grand
nombre d'erreurs critiques en ce moment sur le serveur, on ne reboot pas, bien
trop dangereux.

Ils requêtent ensuite [etcd][20] qui, d'après ce que j'ai compris, leur permet
de d'obtenir un [mutex][21] distribué pour savoir si ce serveur peut reboot. Si
le lock est dispo, le serveur le prends, sinon le script s'arrête et
recommencera dans 1mn, au prochain passage du `cron`.

Au reboot, le serveur release son lock et les autres serveurs du pool peuvent
alors le récupérer pour faire la même danse et se rebooter aussi.

Bien sur, avant de tester tout ça en production, ils ont testé dans leur
environnement de staging, qui est une copie conforme de l'env de prod et qui
reçoit une copie en miroir de toutes les requêtes que la prod reçoit aussi. Ils
ont laissé tourné comme ça pendant plusieurs semaines, pour s'assurer que tout
fonctionnait bien avant de déployer en prod une fois confiants.

Finalement, leur système marche bien et ils en sont très content. Depuis ils
l'ont un peu amélioré, en passant le `cron` à 5mn plutôt que 1. 1mn était trop
court et les serveurs n'avaient pas le temps de redémarrer correctement. 5mn
leur laisse plus d'espace et est encore assez court pour que tous les serveurs
puissent reboot avant 9h. Ils ont aussi réécrit une partie du parsing du json
(anciennement en regexp) par du Ruby.

# Scaling Humans

Le deuxième talk était de [David Mytton][22], de [Server Density][23]. David
nous a expliqué comment ils gèrent les astreintes chez Server Density.

Comme tous les bons ops, il commence par rappeler que quelque soit les
précautions qui sont prises, il y aura toujours des moment de downtime, qui
passent au travers des protections automatiques mises en place et doivent être
gérées par des humains.

Et ces humains doivent être d'astreinte, joignables et prêts à intervenir au
plus vite. Pour cela, il utilisent un système de rotation primary/secondary. Les
primary sont prévenus en premier et doivent être dispo pour répondre en quelques
minutes, se logguer sur les serveurs, commencer à comprendre d'où vient l'erreur
et pouvoir passer ces informations à la seconde équipe qui n'arrive qu'au bout
de 30mn si jamais le problème est toujours en production.

Tous les ops d'astreinte ont le droit à une journée off après un call
d'astreinte, pour se reposer et pouvoir reprendre le travail en forme sans avoir
eu une nuit blanche entre les deux.

Plus un système devient complexe et moins il a de chance de tenir dans la tête
d'une seule personne. Et même si c'est le cas, cette personne ne sera peut-être
pas toujours là, ou ne sera pas dispo pour cette urgence. Il est donc
indispensable que l'infra soit documentée, ainsi que les erreurs communes. Cette
documentation doit être searchable facilement et rapidement et surtout, elle ne
doit pas être hébergée sur les mêmes machines que celles qui risquent de planter
(mettre la doc sur [Google Docs][24] est donc une bonne idée).

La doc doit comprendre la liste des personnes à contacter pour les différents
problèmes, que ce soit des développeur en interne, des responsable de la
relation client, ou des contacts chez les différents prestataires. Les
différents codes d'accès aux différents outils doivent aussi être facilement
accessible pour ne pas se retrouver refoulé à l'entrée parce qu'on n'a pas le
bon mot de passe du support ou pas la bonne clé ssh quand les serveurs sont en
train de bruler.

Dans le même ordre d'idée, il faut s'assurer que le canal de communication
principal ([HipChat][25] par exemple) n'utilise pas la même architecture que
celle qui est susceptible de planter ([AWS][26] par exemple). Les moyens de
communications doivent aussi être redondés.

Avant de se lancer tête baissée dans la résolution du problème il y a déjà
3 points qui doivent être respectés. Tout d'abord, et avant tout, de manière
primordiale, ouvrir la checklist et vérifier de bien tout faire, dans l'ordre.
Si une erreur survient le lundi matin alors qu'on est en pleine forme, c'est pas
un soucis, mais si jamais on doit sauver le monde à trois heures du matin après
une soirée arrosée au réveillon, on n'aura pas les idées très claires et suivre
la checklist nous assurera de ne rien oublier et de le faire dans l'ordre.

Ensuite, on se connecte sur une war room dédiée. Celle-ci est destinée
uniquement aux ops en train d'investiguer le problème, on n'y discute pas, on
n'y partage que des infos qui font avancer l'investigation.

On loggue ensuite l'erreur dans un [JIRA][27] ou autre tracker et on balance
l'url dans la room. Comme ça, on a une seule référence de l'erreur, avec un
numéro unique et tout le monde peut bien parler de la même chose. On y poste les
différentes commandes qui ont été exécutées, les pistes, etc de manière à ce
qu'une nouvelle personne qui arrive sur le problème puisse voir en une seule
fois les symptômes et ce qui a été tenté. Elle permettra aussi de se pencher sur
un post-mortem ensuite.

Et seulement après la checklist, la war room et l'issue JIRA se mets on
à investiguer réellement les causes. Comme je le disais, si on tente des
commandes, si on fait des reboots des serveurs, on le note dans l'issue.

On prévient aussi nos utilisateurs aussi souvent qu'on le peut de l'avancée de
nos investigations, ce qu'on suppose, ce dont on est sur, ce qu'on va faire et
un ETA si on en a un. Il n'y a rien de pire qu'avoir son site down et de n'avoir
pour seule information qu'une page de status mise à jour il y a plus de 4h qui
dit "on s'en occupe".

Il est aussi utile d'avoir toute l'équipe dispo à portée de main. Tout le monde
ne peut pas vraiment aider en même temps, mais de savoir que si besoin on peut
aller voir la personne en charge de telle ou telle partie d'où le bug a pu
provenir et qu'elle sera dispo là tout de suite pour aider à corriger, c'est un
gros plus.

Et une fois que la catastrophe est endiguée, on laisse couler quelques jours
pour se reposer, pour répondre aux clients, et on se retrouve pour faire un
post-mortem. On raconte ce qu'il s'est passé, les causes, les conséquences, les
solutions, et ce qu'on peut faire pour que cela ne se reproduise plus.

On peut ensuite poster ça publiquement sur le blog de la société. Selon
l'affinité technique du public on peut rentrer plus ou moins dans les détails,
mais au moins on le fait en interne au reste de l'équipe.  [Etsy][28],
[Heroku][29], [Amazon][30] et plein d'autres font ça régulièrement: ce qu'il
s'est passé, pourquoi, ce qu'ils ont essayé, ce qui a marché, ce qui n'a pas
marché et les solutions pour la suite.

Un bon guide de savoir-vivre de l'astreinte, concentré sur le coté humain, sur
la communication, savoir expliquer ce qu'il se passe, pas mal de bonnes idées.

# Worst and best in time series data

[Paul Dix][31], d'[InfluxDB][32] nous a ensuite parlé de time series et des
challenges que leur stockage comportait pour des bases de données classiques.

Déjà, un petit rappel sur ce qu'on appelle une time series. Un exemple concret
et parlant sont des lignes de log par exemple. C'est un ensemble de données
(url, host, temps de réponse, etc) lié à un timestamp. Ces données arrivent les
unes après les autres à un serveur. Elles peuvent arriver de manière régulière
(toutes les X minutes) ou de manière irrégulière (en réponse à des événements).

Réussir à modéliser ce genre de données dans une base de donnée (NoSQL ou
relationnelle) classique est très compliqué. Mais si on s'intéresse aux
spécificités de ce genre de data, on se rends compte qu'elles ont des
propriétés très spéciales dont on peut tirer parti.

Tout d'abord, elles ont un nombre d'écritures très importants. Les logs arrivent
tout le temps, sans jamais s'arrêter. Plus on veut tracker de metrics, plus on
va recevoir de logs. Elles ont aussi besoin de pas mal de lectures, vu que les
utilisateurs vont avoir besoin de voir les metrics enregistrées, générer des
graphs, voir les évolutions sur plusieurs minutes, heures, jours, semaines,
mois.

Si on stocke l'ensemble des logs qu'on reçoit, on se retrouve en plus avec des
quantités de données astronomiques et plus de place pour les stocker et encore
moins pour réussir à faire des queries performantes sur un tel dataset.

Mais si on y regarde de plus près, on se rends compte que notre donnée perds de
son intérêt au fil du temps. Les logs de l'instant présent sont plus importants
que ceux d'il y a deux ans. Et on peut se permettre d'effectuer un map/reduce
sur nos anciennes datas pour ne garder que les metrics agrégées extraites de nos
milliers de logs sans en garder le détail précis.

On remarque aussi que les time series ne font jamais d'update de donnée
existante. On ne va jamais modifier un log existant, on va juste en ajouter des
nouveaux, et en plus on va toujours les ajouter récemment. Cette particularité
est très importante parce que cela veut dire qu'on peut se permettre de
supprimer de gros chunks de données dans le passé sans gêner la consistency de
ce qu'on a dans le présent.

Le simple fait de ne pas faire d'update de donnée signifie qu'on ne peut pas
avoir deux ordres qui demandent de modifier la même donnée de deux manières
différentes et qui nous demanderaient de savoir lequel des deux est arrivé en
premier. On n'a pas ce problème, on se contente d'ajouter les logs comme ils
viennent. On peut donc facilement faire de gros calculs sur des blocs entiers de
donnée ancienne en ayant la certitude qu'un ordre ne va venir ajouter ou
modifier un de nos items pendant qu'on fait notre calcul.

Et même dans le cas où notre base est distribuée sur plusieurs serveurs et que
pour une raison ou une autre un des serveurs n'a pas reçu l'ensemble des ordres
d'écriture, il nous est facile de reconstituer le set manquant en faisant la
différence avec ce que les autres ont reçu.

Au final, le talk était un très bon moyen de mettre en évidence les spécificités
des time series et de ce qu'elles impliquent en terme de type de base de
données. Même si son nom n'a jamais été cité de tout le talk, j'imagine
qu'[InfluxDB][33] doit résoudre l'ensemble des problèmes
évoqués.

# Borg

[John Wilkes][34], de Google, nous a parlé de [Borg][35], l'outil utilisé chez
Google qui permet de faire de la parallélisation de tâches sur de multiples
machines/cœurs/threads/whatever.

Honnêtement, je ne sais pas si c'était la chaleur ou le fait que le speaker
parlait vite, et de manière assez monocorde mais j'ai piqué du nez à plusieurs
reprises durant le talk et n'ai pas vraiment grand chose à vous raconter sur
celui-ci !

# Lightning Talks

Après la pause déjeuner, on est repartis pour une petite série de lightning
talks. C'est une super idée qui permet de digérer tranquillement sans s'assoupir
devant une longue conférence qui demande de faire travailler des méninges qui
sont en général à ce moment en train de sortir de leur torpeur.

## Doclipser

Le premier talk était sur un projet assez bizarre, de faire tourner [Docker sous
Eclipse][36]. De ce que j'ai compris, ça permet d'ajouter de la coloration
syntaxique et un vérificateur de syntaxe aux `Dockerfile`, ainsi que de lancer
des images en quelques clics depuis son IDE.

J'avoue que ça m'a pas paru plus facile à utiliser que de taper `docker run
whatever`, mais ça doit être parce qu'Eclipse ne m'a jamais paru facile.

## Your system is distributed

[Sam Bessalah][37] (que je croise souvent en meetup et avec qui j'échange juste
quelques mots), m'a particulièrement surpris avec un sujet super intéressant sur
la place des recherches académiques au sein des différents projets open-source.

Il trouve que bien des projets mentent bien souvent par omission, en considérant
que le réseau est fiable, que la latence n'existe pas, que nous avons une bande
passante illimitée, etc.

Ces complexités existent, il ne faut pas fermer les yeux en se convainquant
qu'il n'y a pas de problèmes, il faut penser un système qui les englobe et qui
sait les gérer. De la même manière, réussir à mettre au point un algo de
consensus dans un système distribué n'est pas quelque chose de simple.

Et c'est tellement peu simple qu'il existe des recherches sur le sujet, des
papiers de plusieurs centaines de pages qui expliquent mathématiquement les
problèmes et leurs différentes solutions. Mais bien souvent, tout cela passe au
dessus de la tête des développeurs qui trouvent ça trop compliqué et recodent
leur propre version.

Il ne faut pas avoir peur des recherches déjà effectuées, au contraire, il faut
continuer à bâtir sur les épaules des géants qui nous ont précédés, il ne faut
pas cantonner ces recherches au monde de la théorie et notre code à celui de la
pratique, les deux ne font partie que d'un seul et même monde.

Good point, j'ai beaucoup apprécié. Même si je suis bien incapable de comprendre
le moindre papier mathématique, je ne me risquerai pas à recoder un algo de
consensus distribué.

## Leader Election with Cassandra

[Matthieu Nantern][38] de [Xebia][39] nous a ensuite lu un texte explicatif sur
[Cassandra][40] et la manière dont le Leader est choisi dans un cluster. Le talk
n'apportait rien de plus que l'[article][41], et était même plus difficile
à suivre.

## Convergent Replicated Data Types

[Dan Brown][42] nous a quand à lui parlé de [la Joconde][43], de
[Jesus-Christ][44] et de [Léonard de Vinci][45]. Ah non, pardon, il nous
a surtout rappelé qu'il était impossible de différencier un échec d'une
lenteur, si on ne définit pas un timeout maximum.

# Consistency and Candy Crush

[Neha Narula][46], dont la bio sur le site se limitait à "_she has a PhD and
she's from Google_" a quand à elle entamé le premier talk de l'après-midi, sur
la cohérence dans une base de donnée et ce que ça implique.

Ce talk était un de plus à nous parler du [CAP Theorem][47], et un de plus
à plus parler de database que ce que je m'attendais à trouver sous le terme de
_scaling_.

On commence par un petit récapitulatif de ce qu'_ACID_ veut dire. 

Le A est pour _atomic_, soit un ordre s'est exécuté, soit il ne s'est pas
exécuté. "_Do. Or do not. There is no try_" comme dirait l'autre.

Le C est justement pour cette Cohérence (Consistency), c'est à dire que le
résultat est correct, selon des règles que nous avons nous même définies.

I pour Isolation, les différentes transactions sont indépendantes et
n'interfèrent pas les unes avec les autres.

Et finalement, le D pour Durable, c'est à dire qu'on peut reconstruire la DB
même après un crash. C'est la lettre qu'on oublie généralement.

Elle nous explique aussi ce qu'on entends sous le terme de serializability,
c'est à dire la capacité d'exécuter un ensemble d'ordres comme si on les
exécutaient les uns après les autres. Attention, ça ne veut pas dire que c'est
comme ça que ça se passe à l'intérieur, ça veut juste dire que le résultat est
le même si on exécute une transaction serialisable que si on exécute les ordres
qui la compose les uns après les autres. 

Dans l'esprit du codeur, c'est plus simple de se représenter une transaction
serialisable, car on n'a alors pas besoin de penser à tous les problèmes
d'enchevêtrement des ordres.

Il y a donc, dans le monde des DB, plusieurs modèles de cohérence. On a d'abord
l'eventual consistency qui dit simplement que si on n'a arrête d'avoir de
nouvelles updates, alors, au bout d'un moment, tout nos nœuds auront la même
data. La difficulté avec ce genre de modèle c'est déjà que la notion de temps
est assez floue et que ce n'est pas évident de savoir exactement quel est le
dernier ordre, et aussi qu'en général les updates ne s'arrêtent jamais
réellement.

De l'autre coté, on a la strict consistency. Les ordres sont exécutés dans
l'ordre où ils sont envoyés, de manière linéaire. Eventual et Strict sont deux
extrêmes d'un spectre avec des tas de variations entre les deux.

Finalement, le take away de sa conférence est que l'eventual consistency c'est
compliqué à gérer et que bien souvent ça amène plus de problèmes que ça n'en
résout. En tout cas, elle conseille de commencer ses développement avec une
cohérence stricte. Et seulement quand cela crée des goulots d'étranglement
tenter de les contourner (et l'eventual consistency peut être une solution mais
pas forcément la seule).

Au final il faut être bien conscient des avantages et des inconvénients des DB
qu'on utilise. Il faut savoir ce qu'on gagne, savoir ce qu'on perds, et ne pas
laisser le système de base de donnée faire ce choix pour nous.


# Ben Firchman

internet as a computer

serveurs gros, de plus en plus peites,c ommodity hardaware, on en mets plein, et
on les splitte en VMs.
maintenant tout dans un contaoner, on n'a pls vraimet besoi de seveurs cmme
avant, chaque container contient tout ce qu'il nécessite, et le serveur 'a rien
d'autre à faire que de devir savoir gérer les containers.

parallelesation de docker, pour ancer une instance pour chaque test, pour chaque
image ou fichier à convertir. En local, ou dans un swarm de serveurs online. Une
suite de taches prendra aussi vite que la plus lente de toutes les taches si on
uilise une instane pour chaque tache.

docker permet de booter des instances trés rapidement et permet e les dégager
rapidement, 

look at Compose, Kubernetes, Mesos

fork-process pourrait devenir une requête TTP vers un swarp d'instances docker
pour répondre rapidement.



# Simon Riggs

REX de 12 ans de développeur de postgresql

a commencé quand il n'y avait pas de sql, obligé de tout opitmiser à la main.
SQL est fantastique, ça permet de spécifier son accès ç ka data, et le systeme
peut alors otpimiser tout seul.

high level pour exprimer ce qu'on veut faire, et le laagage s'occupe de faire
"the right thing"

standard, donc différent choix d'implémetations, et possibilité de changer de
job facilement et de s'adapter.

transactions needed. locking needed.

il ya  des tas de tmanières d enormaliser ses datas, schemaless,
nultidimensionnal, nested relation, etc. L'utilisation d'un seul type est une
erreur, il faut utiliser la manère qui fit le mieux le use case en questions.

une DB doit être en mesure de représenter la data selon différentes manières.
C'est lutisliateur qui choisir la manière dont il la rperensent , pas le
créateur de la DB.

postgre s'est concentré sur les trés nmbreux petits systemes qui l'utilisent,
c'est à dire cex qui n'ont pas besoin d'être distribués (99% de ses
utilisaeurs).

postgreSQL 9.4, store JSON as JSONB, compressé mais indexé
sample system_time en 9.5 pou r demande à ce que la table répondre en luns d'un
certain nmbre de secondes, quelque soit la taille de la donnée. Prends alors des
samples du conenu pour retourner quelque chose ne temps et en heures, mais qui
peut ne pas ere cmléyemet vrai

possible de spécifier le niveau d'ACID à chaque transaction. indiquer si doit
être transacoinnel, attente de a synchronisation
off, very fast immediate return,
local, wiat for write on disk,
ou attente e confirmation réseau

support de plusieurs manière s de stocker les datas, acid, multiple nodes
eventual consistecy. Mais le même code depuis une petite DB vers uene très
rgrande. pas différentes versions de postgresql.

YeSQL.

# Jpense

Call me maybe
@aphyr
Stripe

Api en arc en ciel, du cod d'api en metal, qui s'empile sur des tas e code api,
ruby et dv? S ca crame en as, l'api doit rexter en bonne santé

jpesen ssystem analysis tool.


# Cloudflare
cloudflare est en CDN devat des tas de sites dans le mone.
400TB/day de log, compressed. 4 millions par secondes.
ils font énormement de log, au point qu'ils blaguent en disant qu'ils font du
CDN de manière secondaire (ils optimisent aussi les requêtes cachent, changent
de protoocle, etc)

plein d ebruit autour du signal
ils doivent en extraire le signal du bruit, en tempsréell, car ils ne peuvent
pas tout garder.
charts pour les customers, bandwaith, % de cache, origine, etc. pas besoin des
lignes de log directes, mais les metrics que les utilisateurs veulent voir.
Doivent aussi voir rapidement les attaques de séucirté qui peuvent attaquer les
sites. WSS, SQL injection et autres DDos.
et voir les ayaqies e, teŝ réem; depuis o (?) et vers quel site, etc.

open source et des tools internes écrits en go

sinon, la stack c'est
ngins et luajit
Kafka
Redis
Go
Posgres
streaming algo

ils ne stockent pas grand chose, dnc intérêt à pouvoir utiliser au mieux les
algos de streaming

nginx reverse proxy
fonctionne comme un cache pour otenrl'oriiin. ninx devant pour faire écran pour
cacher un maximum.
mais n seul nins va servir des tas de websites chez eux. donc besoin d'un peu de
code lohoqie à chaque requête pour charger la config du site en question, et
continer ou non la requête http.
pour ça ils utilisent luajit pour ça, qui est trés rapidem. c'est un mélange de
code écrit à la main pour faire un dsl, et de code generated
code en DSL, simple, transofmré en cdode lua difficile à lire, mais facile
à executer par le parser.

un outil en go qui lit les logd ui pssent depuis nginx. il les compresse en lz4,
puis les evoie en batche vers un autre data center, qui ont une connection LTS
ouvert en permanence entre leurs nginx et le server de stoackage des logs
compressés, ils envoient en permanence dessus

cap'n proto
super rapide, 20x plus rapide qe cjson
représentation pour transférer sur le wire, et en mémoire, on peut rajouter des
tas des élments et attributes sna spénalité. comparaison à proto buff ou cjson,
beaucoup plus rapide, parfait si on veut juste envoyer sur le wire

Kafka
envoient les donées en batch toute la journée, ça forme une super queue trés
longues, que lafka arrive à ingérer au fur et à masure. si ca grossit trop, peut
ajouter de plus en plus de nodes kafka pour aiciper. et resilient, au pire nn
garde la queue sans qu'elle ne soit consommées.
garde els donnée sde bacup de quueu pendant 24h
accumule les details des attaques et des requêtes et les renvoie vers postgre,
aggrégés par users

CitusDB
shard de sostgre, permet de requêter des tas de shards de oistgre en eme temps,
pour avoir la dae hebergée sur pusieurs seveurs sans probème. la query est
ensuite enoyée en arallele sur tous le ssahards du cluste.

cusom servers fabriqués par Quanta, avec de sperformanes de ouf. il faut
intégrer 15Go/s
12To de SSD pour l'analytics


# Disque
@antirez

createur de redis
obligé de faire un messqge queu parce que les gens utilisent déjà redis comme un
message queue. ce n'étai pas le premier but de redis, même si c'et possible
avec.

redis sans redis, on garde le squelette et on fair un queue message plutôt qu'un
key/value store
reste en mémoire, peut être persistant de manière optionnell. Garde le ee
protoce et ien sur SD license.

message queu est un type de D trés partciculier et donc plus facile qu'un type
général de db
but principal ici, c'est de faire un jo execution asynchronous

un producer balance des jos à un e queue, par exemple envoyer un message de
welcome quand on se register. On dit ce qu'on veut qu'il fasse, le worker dit ok
j'ai recu l'ordre, puis il processera le message plus tard, quand il pourrat. on
parle de ces messages comme de jobs, même usages, mots interchangeables.

un message peut être 'importe quoi, un jpg, une image, un json, cest du binary
qui est envoyé.

plusieurs queues différentesn acex de snomoms différents
`ADDJOB queue_name job`

par défaut le timeout est de 0, disant qu'on essaie ju'squ'à ce uqe ça marche.
Sinon, s jamais le job n'est pas executé avant la fin du timeout, on le drop.

`GETJOB FROM q1 q2`
pour obtenir les jobs de plusieurs queues, on eut limiter le nombre de jobs
à retourner

si on a processé le mesage, on envoue un `ACKJOB` avec l'id des messages qu'on
a terminé, comme ça supprimé de la queue.
du coup, un essage pet être envoyé à plusieurs leteurs, tant qu'il n'es pas ack.
seulement une fois qu'il sera ack il ne sera plus delivered again

on indique avec un `RETRY` ou `TTL` dans le `ADDJOB` pour dire au bout de
combien de temps on dégage le job de la queue, qu'il est été executé ou non

`REPLICATE 3`, on ne retourne un ack qu'au bout d'une rplication sur au moins
3 nodes, replication sur plusieursnoeuds, configuratbel

le gros catch, c'est que la liste des jobs soit dans l'ordre envoyé. best effort
ordering, 

WIP, si jamais un mail ne peut pas être delivered, il faudrait le savoir. pour
le moment, le job qui foire part dans une autre queue. plus tard, dans la
rodamap, un meilleur moyen de gérer ça. Autorise un `NACK` negative `ACK`
les tools de debug, introspection, suppresion de nods est encore en WIP
still new and thus, instable

# Jeremy Edberg

Your infrasture isnot a unique snowflake and that's ok

overhad, ce qu'on fait dans une boite qui doi être fait et qui n'apporte pas
directement de la valeur

illusion of choixe
poser les bonnes questions en cas de problème. incident review, on apprends ce
qu'il se passe, pas on ne blame pas

what went wrong?
commet le detecter plus vite, etc

build or buy ? il  a sand doute quelque schose outside qui fait déjà 90% de ce
q'on veut faire. NIH syndrome qu'on aime faire, mais qui font sans doute perdre
du temps parce quo'n n'a pas envie de perdre du temps sur les edge cases

monolith ou microservice ?
arguments des deu cotés. microservice permet de faire du reliable. mais mieux de
commencer par du monolith

la plupart de sgrosses compagnis ont commencé en monlolith et on fini en
micrisevcices, netflix, airbnb, pinterest

alors on construit une plateforme pour gerer ce sicroservice,s ils doivent
communiquer. il faut savoir s'arreter pquand la plateforme st good enough, c'est
seulement quand tu fais du sass pour les autres sur ta plateforme que tu dois
faire du great. attention à ne pas perdre d etemps à faire trop d'over eginner
sur le plateforme interne.

et la plupart des boites construisent la même chose, globalement c'es du temps
perdu. on le fait parce qu'il n'y a rien d'autre dispo qui convient à nos
besoin, même si certains cherchent à opensources, comme netlifx, mais ça reste
digicile à utilise t limité quand même au use case de netflix.

conseil, on commnence par un cloud provider. google, azure, amazon. aucun
intérêt à prendre dy hardware dirrectement, pete de temps, pas les compétences
pour gérer ça. les apparences des clouds sont sans doute trompeuses sur la magie
qu'elle offre mais certains des avantages sont biens réels.

CI. choisir entre Jenkins, TeamCity, CircleCi. questions importantes sont que ca
se asse automatiqueme, que ca me dnne des objes ditecement utilisables et que je
puisse faire un deplou en un clic. la tecno a pe d'importance si on a tout ca

Logging and alerts.
inutile de gardes ses logs, prends beaocup de places. plus rapide d eragrder les
logs au mpment ù il ya  un problème pas la eine de d'aller chercher dans un gros
historique. A la rigueur on se mets à les garder uniquement quand il ya  un
problème, mais inutile de tout garder.
on est juste interessé par l'augementation des erruers, pas les logs de succès.
on arde de smetrics au moment ou ça arrive, mais on garde pas

actionable smerics
formulaire de satisfaction sur le formulaire de recherche, 70% de ok, puis on
change l'engine et passe à 90%
on n'influentce pas les utilisateurs, on continue de leur psoer la questio ua fu
et à mesure quo'n fait les modifs, mais sans leur dire qu'on fait des modifs et
on les prévient ensuite. ils seront persuadés avoir la modificaion récemment,
même si c'était il ya  plusieurs mois

network and traffic coding
dplacer le traffic de là ou ça fail autant que possible et possibilité de le
voir facilement sur des cartes qui sont faciles à lire par tout le monde. avir
des outils qui peuevent detecter ça automatiqement et rediriger où il faut

don't build your own load balancer
use HAporyx

service and resource discovery
DNS ets pourri pour a, il ya  du cahcing, des noeuds centralisés
Zookeeper est pa mal, mais reste un SPOF
Netflix a fat une version open source, Eureka, qui est "ok", mais pas great et
diffiicle à utiliser
pour avoir un sisteme distibyé, il fat hasher ses ibjets et les baancer sur les
serveurs. la manière naive est d'utiliser un modulo, mais dès qu'on ajoute un
nouveau tout se déplace. la bonne soltuion est le consistent key hashing qui
distribe sur une clock à 360°, donc quand on ajoute un élément, on ne déplacer
que quelques éléments qui se trouvent aux limites

Automated testing
tout va forcément casser à un moment
"you don't know if youre readu unless you break it yourself, intentionnaly and
repeatidly"
aussi idiot que de faire des backups sans s'assurer qu'ils puissent être
restoarés
Monkyey thoeyr, casse des trucs et voir où ça déconne.
Chaos Monkey, tue de sinstances en production, trouve plein d'ereirs et
maintenant tout le monde construi autour de ça, plus aune machine n'estdevenue
trop importante
Chaos gorilla, détruit des zones entières. permet de tester les fallbacks de
requêtes, voir si les backups foncionnet, si les autres nodes peuvent prendre le
relai sans tomber
Chaos Kong. Tue une région. baance le traffic d'une région vers une autre, teste
le load balancer, detetcion des deads, permet de vérifier que retourne bien sur
les écrans de monitoring. Testé toutes le ssemaine,s sans qu'on ne le voie
vraiment
Latency monkey. Le pire, ne tue pas vraiment, ajoute juste de la latence. plus
dur de trouver un serveur mort, qu'un serveur lent. detecter qaudn c'est lent
dépend de où on appelle et vers qui on appelle.

Security
security doit être au niveau de l'application et du network. ne doit pas
etrepossible de rentrer dans le network, mais mee denda,s ne doit pas avoir
accès à tout. ne doit avoir accès qu'à ce dont il a besoin pour la marche
normale du système.
Don't build your own secuirty tools. Plein de trous, travail d'expert.

Data
il faut plusieurs version de sa donnée, toutes les DB peuvent planter sous
certains conraintes. Il faut avoir la data dans plusieurs datace,ters.


TODO: ADD PICTURES


[1]: http://www.dotscale.io/
[2]: http://www.theatredeparis.com/
[3]: http://www.dotcss.io/
[4]: https://leo-peltier.fr/
[5]: https://twitter.com/joel1di1
[6]: https://twitter.com/julienkirch
[7]: https://twitter.com/clunven
[8]: https://twitter.com/_bpaquet
[9]: https://twitter.com/adamsurak
[10]: https://twitter.com/nagriar
[11]: https://twitter.com/mdespriee
[12]: https://twitter.com/mattbostock
[13]: https://www.gov.uk/
[14]: http://heartbleed.com/
[15]: http://www.ubuntu.com/
[16]: http://en.wikipedia.org/wiki/Bash_%28Unix_shell%29
[17]: https://puppetlabs.com/
[18]: https://www.icinga.org/
[19]: https://www.nagios.org/
[20]: https://github.com/coreos/etcd
[21]: http://en.wikipedia.org/wiki/Mutual_exclusion
[22]: https://twitter.com/davidmytton
[23]: https://www.serverdensity.com/
[24]: https://www.google.com/docs/about/
[25]: https://www.hipchat.com/
[26]: http://aws.amazon.com/
[27]: https://www.atlassian.com/software/jira
[28]: https://www.etsy.com/
[29]: https://dashboard.heroku.com/apps
[30]: http://www.amazon.com/
[31]: https://twitter.com/pauldix
[32]: http://influxdb.com/
[33]: http://influxdb.com/
[34]: https://plus.google.com/+johnwilkes/posts
[35]: https://research.google.com/pubs/pub43438.html
[36]: https://github.com/domeide/doclipser
[37]: https://twitter.com/samklr
[38]: https://twitter.com/mnantern
[39]: http://www.xebia.fr/
[40]: http://cassandra.apache.org/
[41]: http://blog.xebia.fr/2014/12/15/leader-election-avec-cassandra/
[42]: http://www.danbrown.com/
[43]: http://en.wikipedia.org/wiki/Mona_Lisa
[44]: http://en.wikipedia.org/wiki/Jesus
[45]: http://en.wikipedia.org/wiki/Leonardo_da_Vinci
[46]: https://twitter.com/neha
[47]: http://en.wikipedia.org/wiki/CAP_theorem
