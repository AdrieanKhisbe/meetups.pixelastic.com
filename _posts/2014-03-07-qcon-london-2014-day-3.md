## Lessons learned at scaling Twitter

Twitter nous a parlé de leur scaling, qui dans leur cas consiste à découper
leur archi en modules de plus en plus petits et de redonder ceux-ci.

A l'origine, ils avaient une seule application rails, surnommée en interne
"monorail". Une base de donnée mysql derrière, et toute la stack rails pour
gérer le routing, la logique et la présentation. L'archi était faite selon mes
règles de l'art Rails, parfait pour une startup, le code était parfaitement
connu par l'ensemble de l'équipe.

Néanmoins, avec la montée en charge du trafic, mysql est devenu un bottleneck
qui ne pouvait pas scaler correctement. L'appli étant un unique bloc, la
moindre modif de présentation voulait dire redeployer l'ensemble de la stack
sur tous les serveurs. Ca les a forcé à faire du déploiement continu. Malgré
tout, ils ont remarqué des problèmes de concurrence, et de performances.

Ils ont donc décidé de découper leur application en 4 applications, une pour
chaque partie de leur métier : tweets, users, timelines et social graph.

Sur chacun de ces modules, ils ont séparé la présentation de la logique, en
l'exposant sous forme d'API. Ils ont gardé l'appli monorail pour certaines
pages, comme les FAQs.

Globalement, il ont un "TFE" (pas vraiment compris si c'était un acronyme
général, le nom d'un composant ou un truc développé en interne), qui s'occupe
de prendre les requetes qui arrivent et de les renvoyer sur l'appli
correspondante, en gérant lui-même un buffer pour éviter d'overloader une
appli. Dans les fait, c'est bien plus complexe que ça, mais le speaker est
resté générique sur cette brique.

A chaque fois qu'un tweet est posté, il est renvoyé vers les 4 modules, qui
vont chacun le gérer en fonction de leurs besoins. Au final, on a un même
élément (le tweet) qui est stocké à 4 endroits différents.

La timeline twitter fonctionne comme une inbox. Chaque utilisateur possède la
sienne. Ces inbox sont stockées dans une immense base Redis distribuée. Chaque
clée est le twitter id, et chaque valeur est la liste des X derniers tweets. Si
un utilisateur ne vient pas souvent sur twitter, il n'aura pas d'entrée dans la
base Redis (mais un service de fallback peut lui générer), par contre les
utilisateurs réguliers ont leur inbox très facilement accessible.

Bien sur, le Redis ne stocke pas la totalité des tweets, mais juste leurs ids.
Une autre base s'occupe de faire la correlation entre un id et son contenu. Le
service de timeline va donc ensuite s'occuper d'aller récupérer le contenu des
tweets pour constituer la timeline.

Quand un tweet est posté, il est aussi passé dans Ingester, qui est la brique
qui s'occupe d'alimenter la base EarlyBird (shardée) qui s'occupe de la
recherche.

Ils ont aussi un système de push. Quand un tweet est posté, il passe dans leur
application de push, qui va ensuite pusher ce tweet à tous les abonnées.
L'appli à un excellent débit de 30Mo/s en input.

Pour réussir à faire communique tout ce beau monde, ils ont développé
`twitter-server`, un template opensource de serveur, qu'ils utilisent en
interne. Créer une archi composée de ces serveurs leur permet d'obtenir des
metrics générique sur l'ensemble du réseau mais aussi sur chaque noeud. Ca leur
simplifie le discovery, le deploiement et le loadbalancing de tout ca.

Tous leurs serveurs fonctionnent sous forme de fonction. Ils acceptent un input
et retournent un output, et peuvent donc être chainés. L'output est une Future
(c'est le terme dans le monde Scala dont l'équivalent est une Promise en
Javascript). C'est un objet qui représente le résultat du traitement dans le
futur; il n'est pas nécessaire d'attendre le résultat pour pouvoir déjà
commencer à travailler sur celui-ci, il se mettra à jour une fois le traitement
effectué. En utilisant une telle architecture, ils peuvent manipuler des objets
simples représentant des traitement et décider de l'optimisation de ceux-ci
s'ils souhaitent les effectuer en parallele ou en séquence.

Ils ont en interne deux équipes. L'une qui s'occupe du besoin et des appels qui
vont être nécessaire pour faire une telle requete, et une autre équipe qui
s'occupe d'optimiser la concurrence et/ou la parallelisation des Future qui la
compose.

Le simple fait de séparer leur métier en plusieurs modules fait que leurs
équipes sont aussi séparées verticalement. Un peu comme à Spotify, chaque team
est chargée d'une feature et donc touche à tous les aspects de cette feature,
depuis la présentation jusque la configuration de la JVM.

Twitter traite énormément de données, et ils ne peuvent pas facilement
investiguer la suite de requetes d'un utilisateur en particulier si celui-ci
leur remonte un bug. A l'inverse, ils travaillent sur les statistiques de la
totalité du parc, et l'aggrégation d'erreurs. Si un event d'erreur commence
à se produire de manière plus fréquente, ils vont investiguer, mais ils ne
perdent pas de temps à débugguer chaque erreur, ils se contentent de traiter
les errreurs les plus importantes en priorité. Chaque équipe possède un
dashboard temps-réel des métriques des services qu'ils utilisent (pas forcément
uniquement ceux qu'ils ont écrit, mais aussi ceux qu'ils consomment). Ainsi, si
quelque chose déconne, ils peuvent le voir facilement.

Ils ne font pas de tests de performances en interne. Leur masse d'utilisateur
est telle que leurs tests ne pourraient pas réussir à les simuler correctement.
Ils sont aussi tellement dépendant de l'actualité, qui est random, qu'il n'y
a rien de mieux pour eux que de tester directement en prod les performances.
Mais ils font du déploiement progressif, comme Etsy, en n'ouvrant que petit
à petit à de plus en plus d'utilisateurs.

Une question lui a été posée sur ce qui leur posait encore des soucis de
scaling en prod. Il s'avère qu'un si un compte avec des millions de followers
envoie un tweet à un autre compte aussi populaire, tous ceux qui suivent les
deux vont le voir dans leur timeline. Ca fait une sacrée jointure de de
plusieurs millions d'utilisateurs, c'est costaud, mais ça passe.

Il lui a été demandé si le tweet du selfie des Oscards avait posé problème
à leur DB. La réponse est non, mais ce tweet a tellement fait parler de lui que
plein de monde qui ne venaient plus sur Twitter sont revenus, et il a donc
fallu remettre à jour leur timeline oubliée depuis des années, pour tout le
monde en même temps, ce qui a ralentit le service.

Finalement, il nous a expliqué qu'au Japon, il y avait un mot magique dans un
film de Miyazaki qui permettait de détruire la technologie environnante, et le
grand jeu au Japon est que tout le monde tweet ce mot en même temps dès qu'il
est prononcé à la télévision. Heureusement, chacun de ces tweets étant un objet
distinct, ils sont correctement distribués dans leur shards et ça ne détruit
pas Twitter.
